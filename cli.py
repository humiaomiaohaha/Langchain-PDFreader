#!/usr/bin/env python3

import argparse
import sys
from pathlib import Path
from pdf_reader import PDFReader
from llm_interface import LLMInterface
from langchain_community.llms import HuggingFacePipeline
from transformers.pipelines import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½PDFé˜…è¯»å™¨ - å‘½ä»¤è¡Œç‰ˆæœ¬")
    parser.add_argument("--pdf-folder", default="pdfs", help="PDFæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹")
    parser.add_argument("--embedding-model", default="all-MiniLM-L6-v2", help="åµŒå…¥æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„")
    parser.add_argument("--llm-model", default="rule_based", help="è¯­è¨€æ¨¡å‹åç§° (rule_based, local:è·¯å¾„, hf:æ¨¡å‹å, openai)")
    parser.add_argument("--chunk-size", type=int, default=1000, help="æ–‡æ¡£å—å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, default=200, help="æ–‡æ¡£å—é‡å å¤§å°")
    parser.add_argument("--interactive", action="store_true", help="è¿›å…¥äº¤äº’æ¨¡å¼")
    
    args = parser.parse_args()
    
    print("ğŸ“š æ™ºèƒ½PDFé˜…è¯»å™¨ - å‘½ä»¤è¡Œç‰ˆæœ¬")
    print("=" * 50)
    
    try:
        print("æ­£åœ¨åˆå§‹åŒ–PDFé˜…è¯»å™¨...")
        pdf_reader = PDFReader(
            pdf_folder=args.pdf_folder,
            embedding_model=args.embedding_model
        )
        
        print(f"æ­£åœ¨åˆå§‹åŒ–è¯­è¨€æ¨¡å‹: {args.llm_model}")
        llm_interface = LLMInterface(model_name=args.llm_model)
        
        model_info = llm_interface.get_model_info()
        print(f"âœ… è¯­è¨€æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_info['model_type']}")
        
        pdf_folder = Path(args.pdf_folder)
        if not pdf_folder.exists():
            print(f"PDFæ–‡ä»¶å¤¹ {args.pdf_folder} ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            pdf_folder.mkdir(parents=True, exist_ok=True)
            print(f"è¯·å°†PDFæ–‡ä»¶æ”¾å…¥ {args.pdf_folder} æ–‡ä»¶å¤¹ä¸­")
            return
        
        print("æ­£åœ¨åŠ è½½PDFæ–‡ä»¶...")
        documents = pdf_reader.load_pdfs()
        
        if not documents:
            print(f"åœ¨ {args.pdf_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            print("è¯·å°†PDFæ–‡ä»¶æ”¾å…¥è¯¥æ–‡ä»¶å¤¹ä¸­")
            return
        
        print(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        split_docs = pdf_reader.split_documents(
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap
        )
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(split_docs)} ä¸ªå—")
        
        print("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        vectorstore = pdf_reader.create_vectorstore(split_docs)
        
        print("æ­£åœ¨åˆ›å»ºé—®ç­”é“¾...")
        llm_interface.create_qa_chain(vectorstore)
        
        print("âœ… åˆå§‹åŒ–å®Œæˆï¼")
        
        if args.interactive:
            interactive_mode(llm_interface)
        else:
            question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (æˆ–æŒ‰Ctrl+Cé€€å‡º): ")
            answer = llm_interface.ask_question(question)
            print(f"\nç­”æ¡ˆ: {answer['answer']}")
            
            if answer['source_documents']:
                print("\nç›¸å…³æ–‡æ¡£ç‰‡æ®µ:")
                for i, doc in enumerate(answer['source_documents']):
                    print(f"\næ–‡æ¡£ {i+1}: {doc['metadata'].get('source', 'Unknown')}")
                    print(f"é¡µç : {doc['metadata'].get('page', 'Unknown')}")
                    print(f"å†…å®¹: {doc['content'][:200]}...")
            
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ å†è§ï¼")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)

def interactive_mode(llm_interface):
    print("\nğŸ¯ è¿›å…¥äº¤äº’æ¨¡å¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("-" * 50)
    
    while True:
        try:
            question = input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if question.lower() == 'help':
                print_help()
                continue
            
            if not question:
                continue
            
            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
            result = llm_interface.ask_question(question)
            
            if result["error"]:
                print(f"âŒ é”™è¯¯: {result['error']}")
            else:
                print(f"\nğŸ’¡ ç­”æ¡ˆ: {result['answer']}")
                
                if result["source_documents"]:
                    print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£ç‰‡æ®µ ({len(result['source_documents'])} ä¸ª):")
                    for i, doc in enumerate(result['source_documents']):
                        print(f"\n--- æ–‡æ¡£ {i+1} ---")
                        print(f"æ¥æº: {doc['metadata'].get('source', 'Unknown')}")
                        print(f"é¡µç : {doc['metadata'].get('page', 'Unknown')}")
                        print(f"å†…å®¹: {doc['content'][:300]}...")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {e}")

def print_help():
    help_text = """
ğŸ“– å¸®åŠ©ä¿¡æ¯:

åŸºæœ¬å‘½ä»¤:
- ç›´æ¥è¾“å…¥é—®é¢˜: ç³»ç»Ÿä¼šåŸºäºPDFå†…å®¹å›ç­”æ‚¨çš„é—®é¢˜
- help: æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
- quit/exit/q: é€€å‡ºç¨‹åº

ä½¿ç”¨æŠ€å·§:
1. é—®é¢˜è¦å…·ä½“æ˜ç¡®ï¼Œè¿™æ ·èƒ½è·å¾—æ›´å‡†ç¡®çš„ç­”æ¡ˆ
2. å¯ä»¥è¯¢é—®PDFä¸­çš„å…·ä½“å†…å®¹ã€æ¦‚å¿µè§£é‡Šç­‰
3. ç³»ç»Ÿä¼šæ˜¾ç¤ºç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µä½œä¸ºå‚è€ƒ

ç¤ºä¾‹é—®é¢˜:
- "è¿™ä¸ªæ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆï¼Ÿ"
- "è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
- "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›æŠ€æœ¯ï¼Ÿ"
"""
    print(help_text)

if __name__ == "__main__":
    main() 