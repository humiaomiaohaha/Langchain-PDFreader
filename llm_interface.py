import logging
from typing import List, Dict, Any
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
import json

logger = logging.getLogger(__name__)

class LLMInterface:
    def __init__(self, model_name: str = "rule_based"):
        self.model_name = model_name
        self.llm = None
        self.qa_chain = None
        
        self._init_llm()
    
    def _init_llm(self):
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ–è¯­è¨€æ¨¡åž‹: {self.model_name}")
            
            if self.model_name.startswith("local:"):
                self._init_local_transformers()
            elif self.model_name in ["openai", "gpt-3.5-turbo", "gpt-4"]:
                self._init_openai_api()
            elif self.model_name.startswith("hf:"):
                self._init_huggingface_model()
            else:
                self._init_rule_based()
                
        except Exception as e:
            logger.error(f"è¯­è¨€æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"âŒ è¯­è¨€æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ðŸ”„ å›žé€€åˆ°è§„åˆ™ç³»ç»Ÿ")
            self._init_rule_based()
    
    def _init_local_transformers(self):
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            model_path = self.model_name.replace("local:", "")
            logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡åž‹: {model_path}")
            
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            class LocalLLM:
                def __init__(self, model, tokenizer):
                    self.model = model
                    self.tokenizer = tokenizer
                
                def __call__(self, prompt: str) -> str:
                    inputs = self.tokenizer(prompt, return_tensors="pt")
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_length=512,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    return response.replace(prompt, "").strip()
            
            self.llm = LocalLLM(model, tokenizer)
            logger.info("æœ¬åœ°Transformersæ¨¡åž‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æœ¬åœ°Transformersæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_openai_api(self):
        try:
            from langchain_community.llms import OpenAI
            
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("æœªè®¾ç½®OPENAI_API_KEYçŽ¯å¢ƒå˜é‡")
            
            self.llm = OpenAI(
                model_name="gpt-3.5-turbo",
                temperature=0.1,
                openai_api_key=api_key
            )
            logger.info("OpenAI APIè¯­è¨€æ¨¡åž‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"OpenAI APIåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_huggingface_model(self):
        try:
            from langchain_community.llms import HuggingFacePipeline
            from transformers import AutoTokenizer, AutoModelForCausalLM
            from transformers.pipelines import pipeline
            
            model_name = self.model_name.replace("hf:", "")
            logger.info(f"æ­£åœ¨åŠ è½½HuggingFaceæ¨¡åž‹: {model_name}")
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto",
                    trust_remote_code=True,
                    local_files_only=True
                )
                print(f"âœ… ä»Žæœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡åž‹: {model_name}")
            except Exception as e:
                print(f"æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½: {e}")
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
            
            self.llm = HuggingFacePipeline(pipeline=pipe)
            logger.info("HuggingFaceæ¨¡åž‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"HuggingFaceæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_rule_based(self):
        logger.info("ä½¿ç”¨è§„åˆ™ç³»ç»Ÿä½œä¸ºè¯­è¨€æ¨¡åž‹")
        
        try:
            from langchain_community.llms import FakeListLLM
            
            responses = [
                "æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ç›¸å…³ä¿¡æ¯ã€‚",
                "åŸºäºŽPDFæ–‡æ¡£çš„åˆ†æžï¼Œç›¸å…³å†…å®¹å¦‚ä¸‹ï¼š",
                "æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼Œç­”æ¡ˆå¦‚ä¸‹ï¼š",
                "åŸºäºŽæ–‡æ¡£å†…å®¹ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š",
                "æ ¹æ®PDFæ–‡æ¡£ï¼Œç›¸å…³å†…å®¹æ˜¯ï¼š"
            ]
            
            self.llm = FakeListLLM(responses=responses)
            logger.info("è§„åˆ™ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"è§„åˆ™ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            class SimpleLLM:
                def __call__(self, prompt: str) -> str:
                    return f"åŸºäºŽæ–‡æ¡£å†…å®¹ï¼Œ{prompt}çš„ç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n[è¿™é‡Œä¼šæ˜¾ç¤ºä»ŽPDFä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹]"
            
            self.llm = SimpleLLM()
    
    def create_qa_chain(self, vectorstore):
        try:
            if not self.llm:
                raise ValueError("è¯­è¨€æ¨¡åž‹æœªåˆå§‹åŒ–")
            
            template = """{context}

Q: {question}
A:"""

            prompt = PromptTemplate(
                template=template,
                input_variables=["context", "question"]
            )
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
                chain_type_kwargs={"prompt": prompt}
            )
            
            logger.info("é—®ç­”é“¾åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé—®ç­”é“¾å¤±è´¥: {e}")
            raise
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        try:
            if not self.qa_chain:
                raise ValueError("é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨create_qa_chain")
            
            result = self.qa_chain.invoke({"query": question})
            
            answer = result.get("result", "")
            if not answer or answer.strip() == "":
                answer = "æ¨¡åž‹æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå›žç­”ï¼Œè¯·å°è¯•é‡æ–°æé—®æˆ–ä½¿ç”¨ä¸åŒçš„æ¨¡åž‹ã€‚"
            
            return {
                "answer": answer,
                "source_documents": self._format_source_documents(result.get("source_documents", [])),
                "error": None
            }
            
        except Exception as e:
            logger.error(f"æé—®å¤±è´¥: {e}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºçŽ°é”™è¯¯: {e}",
                "source_documents": [],
                "error": str(e)
            }
    
    def _format_source_documents(self, docs) -> List[Dict[str, Any]]:
        formatted_docs = []
        for doc in docs:
            formatted_docs.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })
        return formatted_docs
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "model_type": type(self.llm).__name__,
            "is_available": self.llm is not None
        } 